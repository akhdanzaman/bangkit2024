{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                       cleaned_text\n",
      "0      2  aa hth here cheerful merry new year peep waana...\n",
      "1      0  aaaa help keep trying talk boy sits close lunc...\n",
      "2      1  aaaaaaaaa cant fucking cry want cry eye wont l...\n",
      "3      1  aaaaaaaaaaaaaaaaaaaaaah wish didnt act fake en...\n",
      "4      0  aaaaaaaaaahhhhhhh feel like fault feel like fa...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Memuat dataset\n",
    "df = pd.read_csv('cleaned-text-2.csv')\n",
    "\n",
    "# Melihat beberapa baris pertama dari dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "import re \n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        # Lowercase text\n",
    "        text = text.lower()\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    else:\n",
    "        text = str(text)  # Convert non-string values to string\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Remove rows with empty or NaN text\n",
    "df = df.dropna(subset=['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-string values in 'object32':\n",
      " Empty DataFrame\n",
      "Columns: [label, cleaned_text]\n",
      "Index: []\n",
      "\n",
      "DataFrame after converting 'object32' to string:\n",
      "        label                                       cleaned_text\n",
      "0          2  aa hth here cheerful merry new year peep waana...\n",
      "1          0  aaaa help keep trying talk boy sits close lunc...\n",
      "2          1  aaaaaaaaa cant fucking cry want cry eye wont l...\n",
      "3          1  aaaaaaaaaaaaaaaaaaaaaah wish didnt act fake en...\n",
      "4          0  aaaaaaaaaahhhhhhh feel like fault feel like fa...\n",
      "...      ...                                                ...\n",
      "52622      0  zoloft wellbutrin im feeling angry everything ...\n",
      "52623      0  zoloft year ago started taking zoloft anxiety ...\n",
      "52624      0  zoloftdid help anxiety im probably going get z...\n",
      "52625      0  zoned cut self harm never far cutting never do...\n",
      "52626      0  zoning social situation lot time im around lot...\n",
      "\n",
      "[52627 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identify non-string values\n",
    "non_string_values = df[df['cleaned_text'].apply(lambda x: not isinstance(x, str))]\n",
    "print(\"Non-string values in 'object32':\\n\", non_string_values)\n",
    "\n",
    "# Convert the entire column to string\n",
    "df['cleaned_text'] = df['cleaned_text'].astype(str)\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"\\nDataFrame after converting 'object32' to string:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label            int64\n",
       "cleaned_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3000\n",
      "2    3000\n",
      "1    3000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Menentukan jumlah sampel untuk setiap label\n",
    "n_samples = 3000\n",
    "\n",
    "# Memisahkan dataset berdasarkan label\n",
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "df_label_2 = df[df['label'] == 2]\n",
    "\n",
    "# Melakukan downsampling atau upsampling jika perlu untuk mendapatkan 3000 sampel dari setiap label\n",
    "df_label_0_sampled = resample(df_label_0, replace=True, n_samples=n_samples, random_state=123)\n",
    "df_label_1_sampled = resample(df_label_1, replace=True, n_samples=n_samples, random_state=123)\n",
    "df_label_2_sampled = resample(df_label_2, replace=True, n_samples=n_samples, random_state=123)\n",
    "\n",
    "# Menggabungkan kembali dataset yang telah di-sample\n",
    "df_downsampled = pd.concat([df_label_0_sampled, df_label_1_sampled, df_label_2_sampled])\n",
    "\n",
    "# Mengacak ulang dataset\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "print(df_downsampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Menggunakan tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fungsi untuk tokenisasi\n",
    "def encode(text, label):\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encoded_text['input_ids'][0],\n",
    "        'attention_mask': encoded_text['attention_mask'][0],\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "# Menerapkan tokenisasi pada dataset\n",
    "encoded_data = df_downsampled.apply(lambda x: encode(x['cleaned_text'], x['label']), axis=1)\n",
    "\n",
    "# Membuat TensorFlow dataset\n",
    "input_ids = tf.stack(encoded_data.apply(lambda x: x['input_ids']))\n",
    "attention_masks = tf.stack(encoded_data.apply(lambda x: x['attention_mask']))\n",
    "labels = tf.convert_to_tensor(df_downsampled['label'].values)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks\n",
    "}, labels))\n",
    "\n",
    "# Membagi dataset menjadi train dan validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "train_dataset = train_dataset.batch(32)\n",
    "val_dataset = val_dataset.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "225/225 [==============================] - 2901s 13s/step - loss: 0.5750 - accuracy: 0.7561 - val_loss: 0.4403 - val_accuracy: 0.8317\n",
      "Epoch 2/3\n",
      "225/225 [==============================] - 2726s 12s/step - loss: 0.3304 - accuracy: 0.8846 - val_loss: 0.4470 - val_accuracy: 0.8361\n",
      "Epoch 3/3\n",
      "225/225 [==============================] - 2721s 12s/step - loss: 0.2206 - accuracy: 0.9296 - val_loss: 0.5366 - val_accuracy: 0.8278\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['label'].unique()))\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(optimizer=Adam(learning_rate=5e-5), loss=model.hf_compute_loss, metrics=['accuracy'])\n",
    "\n",
    "# Melatih model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('modelbert\\\\tokenizer_config.json',\n",
       " 'modelbert\\\\special_tokens_map.json',\n",
       " 'modelbert\\\\vocab.txt',\n",
       " 'modelbert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('modelbert')\n",
    "tokenizer.save_pretrained('modelbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 203s 4s/step - loss: 0.5366 - accuracy: 0.8278\n",
      "Validation Loss: 0.5365952849388123\n",
      "Validation Accuracy: 0.8277778029441833\n"
     ]
    }
   ],
   "source": [
    "# Menghitung akurasi pada validation dataset\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 211s 4s/step\n",
      "Mean Absolute Error (MAE): 0.20277777777777778\n",
      "Mean Squared Error (MSE): 0.2638888888888889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Mendapatkan prediksi dari model\n",
    "predictions = model.predict(val_dataset)\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Mendapatkan label sebenarnya dari validation dataset\n",
    "true_labels = []\n",
    "for _, labels in val_dataset:\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "# Menghitung MAE dan MSE\n",
    "mae = mean_absolute_error(true_labels, predicted_labels)\n",
    "mse = mean_squared_error(true_labels, predicted_labels)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Squared Error (MSE): {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
